{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate -U\n",
    "!pip install -q simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs, MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
    "from urllib import request\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import DebertaTokenizer, DebertaForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.functional as F\n",
    "import tqdm\n",
    "import data_preprocessing\n",
    "import gensim\n",
    "import transformers\n",
    "import nlpaug\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "from nlpaug.util import Action\n",
    "from nlpaug.util.file.download import DownloadUtil\n",
    "\n",
    "# Disable wandb authorization request\n",
    "os.environ[\"WANDB_START_METHOD\"]=\"thread\"\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "# prepare logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# check gpu\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "device = 'cuda' if cuda_available else 'cpu'\n",
    "\n",
    "print('Cuda available?', cuda_available)\n",
    "\n",
    "PREPROCESSING_MODE = 'MEDIUM' # Choose between BASIC, MEDIUM and HEAVY\n",
    "LOADING_MODE = 'k' # mode can be 'c', 'k', 'ck', ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    data = data_preprocessing.remove_h_tags(data)\n",
    "    data = data_preprocessing.remove_ampersands(data)\n",
    "    data = data_preprocessing.remove_mentions(data)\n",
    "    if PREPROCESSING_MODE != 'BASIC':\n",
    "        data = data_preprocessing.lowercase(data)\n",
    "    data = data_preprocessing.remove_contractions(data)\n",
    "    if PREPROCESSING_MODE == 'HEAVY':\n",
    "        data = data_preprocessing.remove_multiple_quotations(data)\n",
    "    data = data_preprocessing.remove_extra_spaces(data)\n",
    "    return data\n",
    "\n",
    "def get_rows(dataframe, data):\n",
    "    rows = [] # will contain par_id, label and text\n",
    "    for idx in range(len(dataframe)):\n",
    "        parid = dataframe.par_id[idx]\n",
    "        instance = data.loc[data.par_id == parid]\n",
    "        keyword = instance.keyword.values[0]\n",
    "        country = instance.country.values[0]\n",
    "        text = instance.text.values[0]\n",
    "        if LOADING_MODE == 'c':\n",
    "            text = country + ' | ' + text\n",
    "        elif LOADING_MODE == 'k':\n",
    "            text = keyword + ' | ' + text\n",
    "        elif LOADING_MODE == 'ck':\n",
    "            text = country + ' | ' + keyword + ' | ' + text\n",
    "        rows.append({\n",
    "            'par_id':parid,\n",
    "            'text':text,\n",
    "            'label':instance.label.values[0]\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def load_data(train_size=0.8, random_state=random_seed):\n",
    "    dpm = DontPatronizeMe('./data', './data')\n",
    "    dpm.load_task1()\n",
    "    trids = pd.read_csv('data/train_semeval_parids-labels.csv')\n",
    "    teids = pd.read_csv('data/dev_semeval_parids-labels.csv')\n",
    "    trids.par_id = trids.par_id.astype(str)\n",
    "    teids.par_id = teids.par_id.astype(str)\n",
    "    data = dpm.train_task1_df\n",
    "    data = preprocess_data(data)\n",
    "\n",
    "    rows_train_val = get_rows(trids, data)\n",
    "    rows_test = get_rows(teids, data)\n",
    "\n",
    "    rows_train, rows_val = train_test_split(rows_train_val, train_size=train_size, random_state=random_state)\n",
    "    print(len(rows_train), len(rows_val), len(rows_test))\n",
    "\n",
    "    return pd.DataFrame(rows_train), pd.DataFrame(rows_val), pd.DataFrame(rows_test)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        train_input_ids = self.encodings.input_ids[idx]\n",
    "        train_token_type_ids = self.encodings.token_type_ids[idx]\n",
    "        train_attention_mask = self.encodings.attention_mask[idx]\n",
    "        train_labels = self.encodings.label[idx]\n",
    "        return {\n",
    "            'input_ids': train_input_ids,\n",
    "            'token_type_ids': train_token_type_ids,\n",
    "            'attention_mask': train_attention_mask,\n",
    "            'labels': train_labels\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(train_dataset_raw):\n",
    "\t# initialising the augmentor with \"Glove\"\n",
    "\tcaug = naw.ContextualWordEmbsAug(\n",
    "\t\t\t# option to choose from is \"word2vec\", \"glove\" or \"fasttext\"\n",
    "\t\t\tmodel_path='distilroberta-base',\n",
    "\n",
    "\t\t\t# options available are insert or substitute\n",
    "\t\t\taction='substitute')\n",
    "\ttrain_dataset_raw_positive = train_dataset_raw[train_dataset_raw.label == 1]\n",
    "\n",
    "\tfor idx, row in train_dataset_raw_positive.iterrows():\n",
    "\t\ttrain_dataset_raw.at[idx, 'text'] = caug.augment(row.text)\n",
    "\n",
    "\ttrain_dataset_raw = pd.concat([train_dataset_raw, train_dataset_raw_positive])\n",
    "\t#print(train_dataset_raw)\n",
    "\ttrain_dataset_raw.to_csv('data/augmented_train.csv', index=False)\n",
    "\ttrain_dataset_raw = pd.read_csv('data/augmented_train.csv')\n",
    "\treturn train_dataset_raw\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_raw, eval_dataset_raw, test_dataset_raw = load_data(train_size=0.8)\n",
    "\n",
    "train_dataset_raw = data_augmentation(train_dataset_raw)\n",
    "\n",
    "# Load the DeBERTa tokenizer\n",
    "tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "train_text = train_dataset_raw.text.values\n",
    "eval_text = eval_dataset_raw.text.values\n",
    "test_text = test_dataset_raw.text.values\n",
    "\n",
    "encoding_train = tokenizer(train_text.tolist(), return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "encoding_eval = tokenizer(eval_text.tolist(), return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "encoding_test = tokenizer(test_text.tolist(), return_tensors='pt', padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "encoding_train['label'] = torch.tensor([[0,1] if x == 1 else [1,0] for x in train_dataset_raw['label'].tolist()], dtype=torch.float32)\n",
    "encoding_eval['label'] = torch.tensor([[0,1] if x == 1 else [1,0] for x in eval_dataset_raw['label'].tolist()], dtype=torch.float32)\n",
    "encoding_test['label'] = torch.tensor([[0,1] if x == 1 else [1,0] for x in test_dataset_raw['label'].tolist()], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the CustomDataset class\n",
    "train_dataset = CustomDataset(encoding_train)\n",
    "eval_dataset = CustomDataset(encoding_eval)\n",
    "test_dataset = CustomDataset(encoding_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "max_length = 192\n",
    "batch_size = 32\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    labels = np.argmax(labels, axis=-1)\n",
    "    f1 = f1_score(labels, predictions, average='binary')\n",
    "    return {'f1': f1}\n",
    "\n",
    "trainingargs = TrainingArguments(\n",
    "    learning_rate=lr,\n",
    "    weight_decay=1e-2,\n",
    "    output_dir='/content/training_results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=None,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    seed=random_seed,\n",
    "    optim='adamw_torch',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=trainingargs,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE CELLS HAVE BEEN RUN USING PREPROCESSING_MODE = 'MEDIUM'\n",
    "assert PREPROCESSING_MODE == 'MEDIUM'\n",
    "\n",
    "trainer.train()\n",
    "# Here the best model has been already loaded\n",
    "trainer.predict(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
